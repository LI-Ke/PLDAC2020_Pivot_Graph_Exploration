{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_PREFIX: /local/naacke\n",
      "/local/naacke/spark-2.4.4-bin-hadoop2.7 exists.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "\n",
    "SPARK_VERSION=\"2.4.4\"\n",
    "SPARK_PREFIX=\"/local/\" + os.environ[\"USER\"]\n",
    "print(\"SPARK_PREFIX:\", SPARK_PREFIX)\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"{SPARK_PREFIX}/spark-{SPARK_VERSION}-bin-hadoop2.7\"\n",
    "\n",
    "\n",
    "if (path.isdir(os.environ[\"SPARK_HOME\"])):\n",
    "    print(os.environ[\"SPARK_HOME\"] + \" exists.\")\n",
    "else:\n",
    "    CLOSER_LOCATION = f\"https://www.apache.org/dyn/closer.lua/spark/spark-{SPARK_VERSION}/spark-{SPARK_VERSION}-bin-hadoop2.7.tgz\"\n",
    "    !wget {CLOSER_LOCATION} -O suggest.html\n",
    "    !wget `grep \"suggest the following mirror\" -A2 suggest.html |grep -Po 'href=\"\\K[^\"]*'` -O {SPARK_PREFIX}/spark-{SPARK_VERSION}-bin-hadoop2.7.tgz\n",
    "    !cd {SPARK_PREFIX} && tar xzf spark-{SPARK_VERSION}-bin-hadoop2.7.tgz && rm spark-{SPARK_VERSION}-bin-hadoop2.7.tgz\n",
    "    !rm suggest.html\n",
    "    !echo \"spark installé dans {os.environ[\"SPARK_HOME\"]}\"\n",
    "    !ls -ld {os.environ[\"SPARK_HOME\"]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /home/naacke/.local/lib/python3.7/site-packages (1.3.0)\n",
      "Requirement already satisfied: graphviz in /home/naacke/.local/lib/python3.7/site-packages (0.13)\n",
      "Requirement already satisfied: ipywidgets in /home/naacke/.local/lib/python3.7/site-packages (7.5.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/lib/python3/dist-packages (from ipywidgets) (4.4.0)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/lib/python3/dist-packages (from ipywidgets) (4.10.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/lib/python3/dist-packages (from ipywidgets) (5.8.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/lib/python3/dist-packages (from ipywidgets) (4.3.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/naacke/.local/lib/python3.7/site-packages (from ipywidgets) (3.5.1)\n",
      "Requirement already satisfied: pexpect in /usr/lib/python3/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.6.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/lib/python3/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets) (5.7.8)\n",
      "@jupyter-widgets/jupyterlab-manager:\u001b[32m enabled \u001b[0m\n",
      "@jupyter-widgets/jupyterlab-sidecar:\u001b[32m enabled \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install findspark\n",
    "!pip3 install graphviz\n",
    "\n",
    "# Requirement:\n",
    "# ==============\n",
    "\n",
    "# WIDGETS and interact \n",
    "#----------------------\n",
    "# ref https://ipywidgets.readthedocs.io/en/latest/user_install.html\n",
    "# !pip3 install ipywidgets\n",
    "!pip3 install ipywidgets\n",
    "# pour le notebook\n",
    "#!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# vérifier extension pour le lab\n",
    "# !jupyter labextension check @jupyter-widgets/jupyterlab-manager\n",
    "!jupyter labextension check @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# if not installed, theN\n",
    "#!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "\n",
    "# SIDECAR\n",
    "# -----------\n",
    "!jupyter labextension check @jupyter-widgets/jupyterlab-sidecar\n",
    "\n",
    "# if not installed, then\n",
    "# pip install sidecar\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-sidecar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# functions\n",
    "from pyspark.sql.functions import *\n",
    "# desc() fucntion\n",
    "import pyspark.sql.functions as fn \n",
    "\n",
    "# timer\n",
    "from timeit import default_timer\n",
    "# start = default_timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import needed for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interaction\n",
    "# from __future__ import print_function\n",
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from graphviz import Digraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start spark engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark application id is: local-1580306473162\n"
     ]
    }
   ],
   "source": [
    "#spark.stop()\n",
    "\n",
    "local = \"local[*]\"\n",
    "appName = \"Essai graph spark\"\n",
    "memory=\"8G\"\n",
    "\n",
    "configLocale = SparkConf().setAppName(appName).setMaster(local).\\\n",
    "set(\"spark.executor.memory\", memory).\\\n",
    "set(\"spark.driver.memory\", memory).\\\n",
    "set(\"spark.sql.catalogImplementation\",\"in-memory\")\n",
    "spark = SparkSession.builder.config(conf = configLocale).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark application id is:\", sc.applicationId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA-DIR:/local/naacke/localfiles/pldac_2020-01-22/arXiv/\n"
     ]
    }
   ],
   "source": [
    "# DATA_DIR = os.environ[\"HOME\"]+ \"/res/EPIQUE/Li_KE/DATA/like_2019-10-04/arXiv.2018-01-19_voc_199820173150/\"\n",
    "DATA_DIR = SPARK_PREFIX + \"/localfiles/pldac_2020-01-22/arXiv/\"\n",
    "\n",
    "print('DATA-DIR:' + DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+\n",
      "|idTopic1|idTopic2|similarity          |\n",
      "+--------+--------+--------------------+\n",
      "|304     |367     |1.324177572352318E-4|\n",
      "+--------+--------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "similarityLinks = spark.read.json(DATA_DIR + 'similarityLinks')\n",
    "similarityLinks.show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---------+------+--------------------+\n",
      "|idTopic|localTopicId|period   |term  |weight              |\n",
      "+-------+------------+---------+------+--------------------+\n",
      "|0      |0           |1998-2000|852487|0.036566569921422064|\n",
      "|0      |0           |1998-2000|919446|0.020184340718255162|\n",
      "|0      |0           |1998-2000|880953|0.028266144039501652|\n",
      "+-------+------------+---------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicNodes = spark.read.json(DATA_DIR + 'topicNodes')\n",
    "# topicNodes.show(1, False)\n",
    "topicNodes.orderBy(\"idTopic\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### topicDictionary_unique_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------------------------------------------------------------------------+\n",
      "|idTopic|topic                                                                                                     |\n",
      "+-------+----------------------------------------------------------------------------------------------------------+\n",
      "|41     |[analysi, system, paramet, vnc, factor, cach, control, form, techniqu, part]                              |\n",
      "|401    |[observ, sourc, scheme, decod, encod, feedback, presenc, wireless sensor network, constrain, numer exampl]|\n",
      "+-------+----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topicDictionary_unique_10 = spark.read.json(DATA_DIR + 'topicDictionary_unique_10')\n",
    "topicDictionary_unique_10.show(2, False)\n",
    "# topicDictionary_unique_10.groupBy().max(\"idTopic\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evolutionPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta=0.7\n",
    "\n",
    "nbTopicPerPeriod = 50\n",
    "\n",
    "pivotDir = str(beta)+\"_\" + str(nbTopicPerPeriod) + \"_0.0_10/\"\n",
    "\n",
    "future = spark.read.json(DATA_DIR + 'evolutionPath/' + pivotDir + \"future\")\n",
    "past = spark.read.json(DATA_DIR + 'evolutionPath/' + pivotDir + \"past\")\n",
    "labels = spark.read.json(DATA_DIR + 'evolutionPath/' + pivotDir + \"labels\")\n",
    "\n",
    "# future.show(1, False)\n",
    "# past.show(1, False)\n",
    "# labels.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#labels.printSchema()\n",
    "labels.createOrReplaceTempView(\"labels\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
